# Toxicity Detection

Toxicity Detection is a machine learning project designed to identify toxic comments using Natural Language Processing techniques. The project uses the [Toxic Comment Classification Challenge dataset](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge) from Kaggle.

## Overview

The project's goal is to create a model capable of classifying user comments based on their toxicity levels. It involves building a multi-label classification model that can categorize comments into various toxicity categories such as toxic, severe toxic, obscene, threat, insult, and identity hate. A Streamlit app is also included to interactively check the toxicity of user-provided comments.

## Requirements

- Python 3.x
- TensorFlow 2.x
- Streamlit

## Installation

* Clone the Repository
* Install the required libraries - `pip install tensorflow streamlit`
* Open your Jupyter Notebook and Upload the cloned foLder

## Setup

* Navigate into the cloned repository.
* Create a virtual environment if you wish - `pythom -m venv .venv`
* Install the libraries mentioned above in virtual enviromnet.
* Run the streamlit app using - `streamlit run toxicity.py`
